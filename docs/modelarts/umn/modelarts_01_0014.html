<a name="modelarts_01_0014"></a><a name="modelarts_01_0014"></a>

<h1 class="topictitle1">Model Training</h1>
<div id="body8662426"><p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p01621130855">In addition to data and algorithms, developers spend a lot of time configuring model training parameters. Model training parameters determine the model's precision and convergence time. Parameter selection is heavily dependent on developers' experience. Improper parameter selection will affect the model's precision or significantly increase the time required for model training.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p41621830456">To simplify AI development and improve development efficiency and training performance, ModelArts offers visualized job management, resource management, and version management and automatically performs hyperparameter optimization based on machine learning and reinforcement learning. It provides automatic hyperparameter tuning policies such as learning rate and batch size, and integrates common models.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p91621930559">Currently, when most developers build models, the models usually have dozens of layers or even hundreds of layers and MB-level or GB-level parameters to meet precision requirements. As a result, the specifications of computing resources are extremely high, especially the computing power of hardware resources, memory, and ROM. The resource specifications on the device side are strictly limited. For example, the computing power on the device side is 1 TFLOPS, the memory size is about 2 GB, and the ROM space is about 2 GB, so the model size on the device side must be limited to 100 KB and the inference delay must be limited to 100 milliseconds.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p1816293011514">Therefore, compression technologies with lossless or near-lossless model precision, such as pruning, quantization, and knowledge distillation, are used to implement automatic model compression and optimization, and automatic iteration of model compression and retraining to control the loss of model precision. The low-bit quantization technology, which eliminates the need for retraining, converts the model from a high-precision floating point to a fixed-point operation. Multiple compression and optimization technologies are used to meet the lightweight requirements of device and edge hardware resources. The model compression technology reduces the precision by less than 1% in specific scenarios.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p2162130555">When the training data volume is large, the training of the deep learning model is time-consuming. In computer vision technology, ImageNet-1k (a classification dataset containing 1,000 image classes, referred to as ImageNet) is a commonly used dataset. If you use a P100 GPU to train a ResNet-50 model on the dataset, it will take nearly one week. This hinders rapid development of deep learning applications. Therefore, the acceleration of deep learning training has always been an important concern to the academia and the industry.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p10162430257">Distributed training acceleration needs to be considered in terms of software and hardware. A single optimization method cannot meet expectations. Therefore, optimization of distributed acceleration is a system project. The distributed training architecture needs to be considered in terms of hardware and chip design. To minimize compute and communication delays, many factors need to be considered, including overall compute specifications, network bandwidth, high-speed cache, power consumption, and heat dissipation of the system, and the relationship between compute and communication throughput.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p14162193020512">The software design needs to combine high-performance hardware features to fully use the high-speed hardware network and implement high-bandwidth distributed communication and efficient local data caching. By using training optimization algorithms, such as hybrid parallel, gradient compression, and convolution acceleration, the software and hardware of the distributed training system can be efficiently coordinated and optimized from end to end, and training acceleration can be implemented in a distributed environment of multiple hosts and cards. ModelArts delivers an industry-leading speedup of over 0.8 for ResNet50 on the ImageNet dataset in the distributed environment with thousands of hosts and cards.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p916253019511">To measure the acceleration performance of distributed deep learning, the following two key indicators are used:</p>
<ul id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_ul154412143281"><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li7545201472810">Throughput, that is, the amount of data processed in a unit time</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li75451914102814">Convergence time, that is, the time required to achieve certain precision</li></ul>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p41623302515">The throughput depends on server hardware (for example, more AI acceleration chips with higher FLOPS processing capabilities and higher communication bandwidth achieve higher throughput), data reading and caching, data preprocessing, model computing (for example, convolution algorithm selection), and communication topology optimization. Except low-bit computing and gradient (or parameter) compression, most technologies improve throughput without affecting model precision. To achieve the shortest convergence time, optimize the throughput and adjust the parameters. If the parameters are not adjusted properly, the throughput cannot be optimized. If the batch size is set to a small value, the parallel performance of model training will be relatively poor. As a result, the throughput cannot be improved even if the number of compute nodes are increased.</p>
<p id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_p151622307519">Users are most concerned about convergence time. The MoXing framework implements full-stack optimization and significantly reduces the training convergence time. For data read and preprocessing, MoXing uses multi-level concurrent input pipelines to prevent data I/Os from becoming a bottleneck. In terms of model computing, MoXing provides hybrid precision calculation, which combines semi-precision and single-precision for the upper layer models and reduces the loss caused by precision calculation through adaptive scaling. Dynamic hyperparameter policies (such as momentum and batch size) are used to minimize the number of epochs required for model convergence.</p>
<div class="section" id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_section1049014212289"><h4 class="sectiontitle">ModelArts High-Performance Distributed Training Optimization</h4><ul id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_ul11626203032814"><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li11163183013515">Automatic hybrid precision to fully utilize hardware computing capabilities</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li121636302057">Dynamic hyperparameter adjustment technologies (dynamic batch size, image size, and momentum)</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li1316316307517">Automatic model gradient merging and splitting</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li15163153010518">Communication operator scheduling optimization based on BP bubble adaptive computing</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li61638307519">Distributed high-performance communication libraries (NStack and HCCL)</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li51633304510">Distributed data-model hybrid parallel</li><li id="modelarts_01_0014__en-us_topic_0284258740_en-us_topic_0168462755_li2062713017286">Training data compression and multi-level caching</li></ul>
</div>
</div>
<div>
<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong> <a href="modelarts_01_0009.html">Basic Knowledge</a></div>
</div>
</div>

