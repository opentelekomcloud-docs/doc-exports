<a name="modelarts_01_0015"></a><a name="modelarts_01_0015"></a>

<h1 class="topictitle1">Model Deployment</h1>
<div id="body8662426"><p id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_p16698024144411">Generally, AI model deployment and large-scale implementation are complex.</p>
<p id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_p1850322719445">ModelArts resolves this issue by deploying a trained model on different devices in various scenarios with only a few clicks. This secure and reliable one-stop deployment is available for individual developers, enterprises, and device manufacturers.</p>
<div class="fignone" id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_fig104181892237"><span class="figcap"><b>Figure 1 </b>Process of deploying a model</span><br><span><img id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_image169391454237" src="en-us_image_0000001110920824.png"></span></div>
<ul id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_ul814611421055"><li id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_li11461442653">The real-time inference service features high concurrency, low latency, and elastic scaling.</li><li id="modelarts_01_0015__en-us_topic_0284258741_en-us_topic_0168462756_li86251938198">Models can be deployed as real-time inference services and batch inference tasks.</li></ul>
</div>
<div>
<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong> <a href="modelarts_01_0009.html">Basic Knowledge</a></div>
</div>
</div>

